{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c675a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nishithreddy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import difflib\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "import contractions\n",
    "\n",
    "import jiwer\n",
    "import nltk \n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "376787fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Folders for reference (.txt) and ASR output (.srt)\n",
    "reference_folder = \"transcripts/original_transcripts\" \n",
    "fw_asr_folder = \"transcripts/faster_whisper_transcripts\"\n",
    "wb_asr_folder = \"transcripts/whisper_base_transcripts\"\n",
    "wt_asr_folder = \"transcripts/whisper_tiny_transcripts\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72ec2fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the reference text files\n",
    "reference_files = glob(os.path.join(reference_folder, \"*.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52352f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILLER_WORDS = set([\n",
    "    'uh', 'um', 'oh', 'ah', 'er', 'mm', 'hm', 'mmhmm','hmm', 'mhm', 'mmhm', 'uhhuh',\n",
    "    'yeah', 'yes', 'no', 'ok', 'okay', 'well', 'like', 'so', 'right',\n",
    "    'you know', 'i mean', '–', 'know', 'think', 'really', 'kind', 'mean', 'sort', 'said'\n",
    "])\n",
    "\n",
    "NUMBER_MAP = {\n",
    "    'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5',\n",
    "    'six': '6', 'seven': '7', 'eight': '8', 'nine': '9', 'ten': '10',\n",
    "    'eleven': '11', 'twelve': '12', 'thirteen': '13', 'fourteen': '14',\n",
    "    'fifteen': '15', 'sixteen': '16', 'seventeen': '17', 'eighteen': '18',\n",
    "    'nineteen': '19', 'twenty': '20', 'thirty': '30', 'forty': '40',\n",
    "    'fifty': '50', 'sixty': '60', 'seventy': '70', 'eighty': '80', 'ninety': '90',\n",
    "    'hundred': '100', 'thousand': '1000'\n",
    "}\n",
    "\n",
    "def normalize(text):\n",
    "    \"\"\"Lowercase, remove punctuation, and collapse whitespace.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    try:\n",
    "        text = contractions.fix(text)\n",
    "    except Exception:\n",
    "        pass # Ignore errors on weird inputs\n",
    "    \n",
    "    try:\n",
    "        words = text.split()\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            # If word is in our map, replace it. Otherwise, keep it.\n",
    "            new_words.append(NUMBER_MAP.get(word, word))\n",
    "        text = \" \".join(new_words)\n",
    "    except Exception:\n",
    "        pass # Ignore errors    # NEW: Remove filler words\n",
    "    # This is a simple way; a more robust way would use regex word boundaries\n",
    "    words = text.split()\n",
    "    non_filler_words = [word for word in words if word not in FILLER_WORDS]\n",
    "    text = \" \".join(non_filler_words)\n",
    "    \n",
    "    text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "690c509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_srt(srt_path):\n",
    "    \"\"\"Extracts only the dialogue text from an SRT subtitle file.\"\"\"\n",
    "    with open(srt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    text_lines = []\n",
    "    # Regex to identify timestamp lines like \"00:00:01,234 --> 00:00:05,678\"\n",
    "    timestamp_re = re.compile(r'\\d{2}:\\d{2}:\\d{2},\\d{3}\\s*-->\\s*\\d{2}:\\d{2}:\\d{2},\\d{3}')\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if re.match(r'^\\d+$', line):  # Skip subtitle number lines\n",
    "            continue\n",
    "        if timestamp_re.match(line): # Skip timestamp lines\n",
    "            continue\n",
    "        text_lines.append(line)\n",
    "        \n",
    "    return \" \".join(text_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "498f7d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Accumulators for error analysis ---\n",
    "wer_scores = []\n",
    "sub_counts = Counter()   # (reference_word, hypothesis_word) -> count\n",
    "ins_counts = Counter()   # inserted_word -> count\n",
    "del_counts = Counter()   # deleted_word -> count\n",
    "\n",
    "# --- Counters for \"meaningful\" errors ---\n",
    "# We'll use these to store errors that DON'T involve a stopword\n",
    "meaningful_sub_counts = Counter()\n",
    "meaningful_ins_counts = Counter()\n",
    "meaningful_del_counts = Counter()\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8c14c5",
   "metadata": {},
   "source": [
    "### Whisper Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e73f5c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lawson-steve-20111111-stereo: WER = 0.138\n",
      "lawson-steve-20111114-stereo: WER = 0.242\n",
      "hedberg-ken-20110929-stereo: WER = 0.117\n",
      "roth-jean-starker-20071031: WER = 0.378\n",
      "lawson-steve-20110919-stereo: WER = 0.215\n",
      "lawson-steve-20110826-stereo: WER = 0.383\n",
      "white-charlie-20110518: WER = 0.297\n",
      "bella-david-20140709: WER = 0.143\n",
      "block-john-20140805: WER = 0.162\n",
      "parr-al-20140618: WER = 0.265\n",
      "robbins-bill-20120327-stereo: WER = 0.194\n",
      "hedberg-ken-20110920-stereo: WER = 0.177\n",
      "coleman-ralph-20140708: WER = 0.250\n",
      "roth-jean-starker-20071113: WER = 0.446\n",
      "mathews-chris-20110902-stereo-final: WER = 0.147\n",
      "strauss-steve-20170307: WER = 0.176\n",
      "hedberg-ken-20110909-stereo: WER = 0.162\n",
      "hedberg-ken-20111020-stereo: WER = 0.176\n"
     ]
    }
   ],
   "source": [
    "for reference_file in reference_files:\n",
    "    filename = os.path.splitext(os.path.basename(reference_file))[0]\n",
    "    asr_file = os.path.join(wb_asr_folder, filename + \".srt\")\n",
    "    \n",
    "    if not os.path.exists(asr_file):\n",
    "        print(f\"SKIPPING: ASR file missing for {filename}\")\n",
    "        continue\n",
    "\n",
    "    # --- Get Reference and Hypothesis Text ---\n",
    "    \n",
    "    # 1. Reference text from the clean .txt file\n",
    "    with open(reference_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        reference_text = f.read()\n",
    "    \n",
    "    # NEW: Remove bracketed text like [laughs] or [unintelligible]\n",
    "    reference_text = re.sub(r'\\[.*?\\]', '', reference_text)\n",
    "    \n",
    "    reference = normalize(reference_text)\n",
    "\n",
    "    # 2. Hypothesis text from the ASR's .srt file\n",
    "    hypothesis_text = extract_text_from_srt(asr_file)\n",
    "    hypothesis = normalize(hypothesis_text)\n",
    "\n",
    "    # --- Calculate WER ---\n",
    "    wer = jiwer.wer(reference, hypothesis)\n",
    "    wer_scores.append((filename, wer))\n",
    "    print(f\"{filename}: WER = {wer:.3f}\")\n",
    "\n",
    "    # --- Detailed Error Analysis using difflib ---\n",
    "    ref_words = reference.split()\n",
    "    hyp_words = hypothesis.split()\n",
    "    sm = difflib.SequenceMatcher(None, ref_words, hyp_words)\n",
    "    \n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == \"replace\":\n",
    "            for ref_word, hyp_word in zip(ref_words[i1:i2], hyp_words[j1:j2]):\n",
    "                # 1. Update total counts\n",
    "                sub_counts[(ref_word, hyp_word)] += 1\n",
    "                \n",
    "                # 2. NEW: Update meaningful counts\n",
    "                # Only count if at least one of the words is NOT a stopword\n",
    "                if ref_word not in stop_words or hyp_word not in stop_words:\n",
    "                    meaningful_sub_counts[(ref_word, hyp_word)] += 1\n",
    "                    \n",
    "        elif tag == \"delete\":\n",
    "            for ref_word in ref_words[i1:i2]:\n",
    "                # 1. Update total counts\n",
    "                del_counts[ref_word] += 1\n",
    "                \n",
    "                # 2. NEW: Update meaningful counts\n",
    "                # Only count if the deleted word is NOT a stopword\n",
    "                if ref_word not in stop_words:\n",
    "                    meaningful_del_counts[ref_word] += 1\n",
    "                    \n",
    "        elif tag == \"insert\":\n",
    "            for hyp_word in hyp_words[j1:j2]:\n",
    "                # 1. Update total counts\n",
    "                ins_counts[hyp_word] += 1\n",
    "                \n",
    "                # 2. NEW: Update meaningful counts\n",
    "                # Only count if the inserted word is NOT a stopword\n",
    "                if hyp_word not in stop_words:\n",
    "                    meaningful_ins_counts[hyp_word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70bdd8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average WER across all files: 0.226\n",
      "\n",
      "Whispers used: whisper Base\n",
      "\n",
      "Top 10 Meaningful Word Substitution Errors:\n",
      "  'pauling' → 'pauline' (50 times)\n",
      "  'id' → 'i' (38 times)\n",
      "  'pauling' → 'pong' (33 times)\n",
      "  'wed' → 'we' (31 times)\n",
      "  'lise' → 'lisa' (29 times)\n",
      "  'rath' → 'rat' (21 times)\n",
      "  'can' → 'could' (21 times)\n",
      "  'i' → 'id' (20 times)\n",
      "  'pauling' → 'plain' (19 times)\n",
      "  'could' → 'can' (19 times)\n",
      "\n",
      "Top 10 Meaningful Word Insertion Errors:\n",
      "  'would' (74 times)\n",
      "  'time' (64 times)\n",
      "  'good' (48 times)\n",
      "  '1' (36 times)\n",
      "  'course' (32 times)\n",
      "  'say' (32 times)\n",
      "  'also' (30 times)\n",
      "  'could' (30 times)\n",
      "  'lot' (29 times)\n",
      "  'little' (28 times)\n",
      "\n",
      "Top 10 Meaningful Word Deletion Errors:\n",
      "  'would' (52 times)\n",
      "  'liked' (14 times)\n",
      "  'time' (12 times)\n",
      "  'could' (9 times)\n",
      "  'says' (8 times)\n",
      "  'little' (8 times)\n",
      "  'us' (8 times)\n",
      "  '1' (7 times)\n",
      "  'interesting' (6 times)\n",
      "  'went' (6 times)\n"
     ]
    }
   ],
   "source": [
    "# --- Summary and Results ---\n",
    "if wer_scores:\n",
    "    avg_wer = sum(score for _, score in wer_scores) / len(wer_scores)\n",
    "    print(f\"\\nAverage WER across all files: {avg_wer:.3f}\")\n",
    "\n",
    "print(\"\\nWhispers used: whisper Base\") # You can change this manually\n",
    "\n",
    "print(\"\\nTop 10 Meaningful Word Substitution Errors:\")\n",
    "for (ref, hyp), count in meaningful_sub_counts.most_common(10):\n",
    "    print(f\"  '{ref}' → '{hyp}' ({count} times)\")\n",
    "\n",
    "print(\"\\nTop 10 Meaningful Word Insertion Errors:\")\n",
    "for word, count in meaningful_ins_counts.most_common(10):\n",
    "    print(f\"  '{word}' ({count} times)\")\n",
    "\n",
    "print(\"\\nTop 10 Meaningful Word Deletion Errors:\")\n",
    "for word, count in meaningful_del_counts.most_common(10):\n",
    "    print(f\"  '{word}' ({count} times)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390dab7c",
   "metadata": {},
   "source": [
    "### Whisper tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9667983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Accumulators for error analysis ---\n",
    "wer_scores = []\n",
    "sub_counts = Counter()   # (reference_word, hypothesis_word) -> count\n",
    "ins_counts = Counter()   # inserted_word -> count\n",
    "del_counts = Counter()   # deleted_word -> count\n",
    "\n",
    "# --- Counters for \"meaningful\" errors ---\n",
    "# We'll use these to store errors that DON'T involve a stopword\n",
    "meaningful_sub_counts = Counter()\n",
    "meaningful_ins_counts = Counter()\n",
    "meaningful_del_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a998f085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lawson-steve-20111111-stereo: WER = 0.160\n",
      "lawson-steve-20111114-stereo: WER = 0.274\n",
      "hedberg-ken-20110929-stereo: WER = 0.137\n",
      "roth-jean-starker-20071031: WER = 0.413\n",
      "lawson-steve-20110919-stereo: WER = 0.225\n",
      "lawson-steve-20110826-stereo: WER = 0.404\n",
      "white-charlie-20110518: WER = 0.358\n",
      "bella-david-20140709: WER = 0.157\n",
      "block-john-20140805: WER = 0.185\n",
      "parr-al-20140618: WER = 0.281\n",
      "robbins-bill-20120327-stereo: WER = 0.216\n",
      "hedberg-ken-20110920-stereo: WER = 0.218\n",
      "coleman-ralph-20140708: WER = 0.288\n",
      "roth-jean-starker-20071113: WER = 0.459\n",
      "mathews-chris-20110902-stereo-final: WER = 0.174\n",
      "strauss-steve-20170307: WER = 0.211\n",
      "hedberg-ken-20110909-stereo: WER = 0.186\n",
      "hedberg-ken-20111020-stereo: WER = 0.203\n"
     ]
    }
   ],
   "source": [
    "for reference_file in reference_files:\n",
    "    filename = os.path.splitext(os.path.basename(reference_file))[0]\n",
    "    asr_file = os.path.join(wt_asr_folder, filename + \".srt\")\n",
    "    \n",
    "    if not os.path.exists(asr_file):\n",
    "        print(f\"SKIPPING: ASR file missing for {filename}\")\n",
    "        continue\n",
    "\n",
    "    # --- Get Reference and Hypothesis Text ---\n",
    "    \n",
    "    # 1. Reference text from the clean .txt file\n",
    "    with open(reference_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        reference_text = f.read()\n",
    "    \n",
    "    # NEW: Remove bracketed text like [laughs] or [unintelligible]\n",
    "    reference_text = re.sub(r'\\[.*?\\]', '', reference_text)\n",
    "    \n",
    "    reference = normalize(reference_text)\n",
    "\n",
    "    # 2. Hypothesis text from the ASR's .srt file\n",
    "    hypothesis_text = extract_text_from_srt(asr_file)\n",
    "    hypothesis = normalize(hypothesis_text)\n",
    "\n",
    "    # --- Calculate WER ---\n",
    "    wer = jiwer.wer(reference, hypothesis)\n",
    "    wer_scores.append((filename, wer))\n",
    "    print(f\"{filename}: WER = {wer:.3f}\")\n",
    "\n",
    "   # --- Detailed Error Analysis using difflib ---\n",
    "    ref_words = reference.split()\n",
    "    hyp_words = hypothesis.split()\n",
    "    sm = difflib.SequenceMatcher(None, ref_words, hyp_words)\n",
    "    \n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == \"replace\":\n",
    "            for ref_word, hyp_word in zip(ref_words[i1:i2], hyp_words[j1:j2]):\n",
    "                # 1. Update total counts\n",
    "                sub_counts[(ref_word, hyp_word)] += 1\n",
    "                \n",
    "                # 2. NEW: Update meaningful counts\n",
    "                # Only count if at least one of the words is NOT a stopword\n",
    "                if ref_word not in stop_words or hyp_word not in stop_words:\n",
    "                    meaningful_sub_counts[(ref_word, hyp_word)] += 1\n",
    "                    \n",
    "        elif tag == \"delete\":\n",
    "            for ref_word in ref_words[i1:i2]:\n",
    "                # 1. Update total counts\n",
    "                del_counts[ref_word] += 1\n",
    "                \n",
    "                # 2. NEW: Update meaningful counts\n",
    "                # Only count if the deleted word is NOT a stopword\n",
    "                if ref_word not in stop_words:\n",
    "                    meaningful_del_counts[ref_word] += 1\n",
    "                    \n",
    "        elif tag == \"insert\":\n",
    "            for hyp_word in hyp_words[j1:j2]:\n",
    "                # 1. Update total counts\n",
    "                ins_counts[hyp_word] += 1\n",
    "                \n",
    "                # 2. NEW: Update meaningful counts\n",
    "                # Only count if the inserted word is NOT a stopword\n",
    "                if hyp_word not in stop_words:\n",
    "                    meaningful_ins_counts[hyp_word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1a15cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average WER across all files: 0.253\n",
      "\n",
      "Whispers used: whisper Tiny\n",
      "\n",
      "Top 10 Meaningful Word Substitution Errors:\n",
      "  'id' → 'i' (46 times)\n",
      "  'wed' → 'we' (35 times)\n",
      "  'pauling' → 'pong' (31 times)\n",
      "  'pauling' → 'poly' (27 times)\n",
      "  'i' → 'id' (24 times)\n",
      "  'can' → 'could' (24 times)\n",
      "  'lise' → 'lisa' (23 times)\n",
      "  'pauling' → 'is' (22 times)\n",
      "  'he' → 'hes' (20 times)\n",
      "  'had' → 'would' (19 times)\n",
      "\n",
      "Top 10 Meaningful Word Insertion Errors:\n",
      "  'would' (61 times)\n",
      "  'time' (60 times)\n",
      "  'good' (44 times)\n",
      "  '1' (38 times)\n",
      "  'course' (36 times)\n",
      "  'could' (30 times)\n",
      "  'also' (29 times)\n",
      "  'say' (27 times)\n",
      "  'little' (26 times)\n",
      "  'probably' (24 times)\n",
      "\n",
      "Top 10 Meaningful Word Deletion Errors:\n",
      "  'would' (50 times)\n",
      "  'liked' (17 times)\n",
      "  '1' (10 times)\n",
      "  'time' (10 times)\n",
      "  'little' (9 times)\n",
      "  'go' (9 times)\n",
      "  'interesting' (7 times)\n",
      "  'could' (7 times)\n",
      "  'say' (7 times)\n",
      "  'talk' (6 times)\n"
     ]
    }
   ],
   "source": [
    "# --- Summary and Results ---\n",
    "if wer_scores:\n",
    "    avg_wer = sum(score for _, score in wer_scores) / len(wer_scores)\n",
    "    print(f\"\\nAverage WER across all files: {avg_wer:.3f}\")\n",
    "\n",
    "print(\"\\nWhispers used: whisper Tiny\") # You can change this manually\n",
    "\n",
    "print(\"\\nTop 10 Meaningful Word Substitution Errors:\")\n",
    "for (ref, hyp), count in meaningful_sub_counts.most_common(10):\n",
    "    print(f\"  '{ref}' → '{hyp}' ({count} times)\")\n",
    "\n",
    "print(\"\\nTop 10 Meaningful Word Insertion Errors:\")\n",
    "for word, count in meaningful_ins_counts.most_common(10):\n",
    "    print(f\"  '{word}' ({count} times)\")\n",
    "\n",
    "print(\"\\nTop 10 Meaningful Word Deletion Errors:\")\n",
    "for word, count in meaningful_del_counts.most_common(10):\n",
    "    print(f\"  '{word}' ({count} times)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71db8b4d",
   "metadata": {},
   "source": [
    "### Faster Whisper Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "209c5765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Accumulators for error analysis ---\n",
    "wer_scores = []\n",
    "sub_counts = Counter()   # (reference_word, hypothesis_word) -> count\n",
    "ins_counts = Counter()   # inserted_word -> count\n",
    "del_counts = Counter()   # deleted_word -> count\n",
    "\n",
    "# --- Counters for \"meaningful\" errors ---\n",
    "# We'll use these to store errors that DON'T involve a stopword\n",
    "meaningful_sub_counts = Counter()\n",
    "meaningful_ins_counts = Counter()\n",
    "meaningful_del_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3bb4dc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lawson-steve-20111111-stereo: WER = 0.131\n",
      "lawson-steve-20111114-stereo: WER = 0.238\n",
      "hedberg-ken-20110929-stereo: WER = 0.111\n",
      "roth-jean-starker-20071031: WER = 0.369\n",
      "lawson-steve-20110919-stereo: WER = 0.230\n",
      "lawson-steve-20110826-stereo: WER = 0.380\n",
      "white-charlie-20110518: WER = 0.284\n",
      "bella-david-20140709: WER = 0.117\n",
      "block-john-20140805: WER = 0.145\n",
      "parr-al-20140618: WER = 0.239\n",
      "robbins-bill-20120327-stereo: WER = 0.176\n",
      "hedberg-ken-20110920-stereo: WER = 0.162\n",
      "coleman-ralph-20140708: WER = 0.216\n",
      "roth-jean-starker-20071113: WER = 0.410\n",
      "mathews-chris-20110902-stereo-final: WER = 0.129\n",
      "strauss-steve-20170307: WER = 0.184\n",
      "hedberg-ken-20110909-stereo: WER = 0.153\n",
      "hedberg-ken-20111020-stereo: WER = 0.175\n"
     ]
    }
   ],
   "source": [
    "for reference_file in reference_files:\n",
    "    filename = os.path.splitext(os.path.basename(reference_file))[0]\n",
    "    asr_file = os.path.join(fw_asr_folder, filename + \".srt\")\n",
    "    \n",
    "    if not os.path.exists(asr_file):\n",
    "        print(f\"SKIPPING: ASR file missing for {filename}\")\n",
    "        continue\n",
    "\n",
    "    # --- Get Reference and Hypothesis Text ---\n",
    "    \n",
    "    # 1. Reference text from the clean .txt file\n",
    "    with open(reference_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        reference_text = f.read()\n",
    "    \n",
    "    # NEW: Remove bracketed text like [laughs] or [unintelligible]\n",
    "    reference_text = re.sub(r'\\[.*?\\]', '', reference_text)\n",
    "    \n",
    "    reference = normalize(reference_text)\n",
    "\n",
    "    # 2. Hypothesis text from the ASR's .srt file\n",
    "    hypothesis_text = extract_text_from_srt(asr_file)\n",
    "    hypothesis = normalize(hypothesis_text)\n",
    "\n",
    "    # --- Calculate WER ---\n",
    "    wer = jiwer.wer(reference, hypothesis)\n",
    "    wer_scores.append((filename, wer))\n",
    "    print(f\"{filename}: WER = {wer:.3f}\")\n",
    "\n",
    " # --- Detailed Error Analysis using difflib ---\n",
    "    ref_words = reference.split()\n",
    "    hyp_words = hypothesis.split()\n",
    "    sm = difflib.SequenceMatcher(None, ref_words, hyp_words)\n",
    "    \n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == \"replace\":\n",
    "            for ref_word, hyp_word in zip(ref_words[i1:i2], hyp_words[j1:j2]):\n",
    "                # 1. Update total counts\n",
    "                sub_counts[(ref_word, hyp_word)] += 1\n",
    "                \n",
    "                # 2. NEW: Update meaningful counts\n",
    "                # Only count if at least one of the words is NOT a stopword\n",
    "                if ref_word not in stop_words or hyp_word not in stop_words:\n",
    "                    meaningful_sub_counts[(ref_word, hyp_word)] += 1\n",
    "                    \n",
    "        elif tag == \"delete\":\n",
    "            for ref_word in ref_words[i1:i2]:\n",
    "                # 1. Update total counts\n",
    "                del_counts[ref_word] += 1\n",
    "                \n",
    "                # 2. NEW: Update meaningful counts\n",
    "                # Only count if the deleted word is NOT a stopword\n",
    "                if ref_word not in stop_words:\n",
    "                    meaningful_del_counts[ref_word] += 1\n",
    "                    \n",
    "        elif tag == \"insert\":\n",
    "            for hyp_word in hyp_words[j1:j2]:\n",
    "                # 1. Update total counts\n",
    "                ins_counts[hyp_word] += 1\n",
    "                \n",
    "                # 2. NEW: Update meaningful counts\n",
    "                # Only count if the inserted word is NOT a stopword\n",
    "                if hyp_word not in stop_words:\n",
    "                    meaningful_ins_counts[hyp_word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7592164d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average WER across all files: 0.214\n",
      "\n",
      "Whispers used: Faster Whisper Base\n",
      "\n",
      "Top 10 Meaningful Word Substitution Errors:\n",
      "  'pauling' → 'pauline' (82 times)\n",
      "  'id' → 'i' (42 times)\n",
      "  'lise' → 'lisa' (27 times)\n",
      "  'i' → 'id' (25 times)\n",
      "  'pauling' → 'pond' (24 times)\n",
      "  'wed' → 'we' (23 times)\n",
      "  'paulings' → 'paulines' (22 times)\n",
      "  'could' → 'can' (21 times)\n",
      "  'rath' → 'rat' (21 times)\n",
      "  'pauling' → 'pong' (18 times)\n",
      "\n",
      "Top 10 Meaningful Word Insertion Errors:\n",
      "  'would' (67 times)\n",
      "  'time' (63 times)\n",
      "  'good' (49 times)\n",
      "  'course' (34 times)\n",
      "  '1' (33 times)\n",
      "  'little' (31 times)\n",
      "  'also' (28 times)\n",
      "  'could' (28 times)\n",
      "  'got' (28 times)\n",
      "  'sure' (27 times)\n",
      "\n",
      "Top 10 Meaningful Word Deletion Errors:\n",
      "  'would' (51 times)\n",
      "  'little' (14 times)\n",
      "  '1' (11 times)\n",
      "  'says' (10 times)\n",
      "  'work' (9 times)\n",
      "  'time' (9 times)\n",
      "  'liked' (8 times)\n",
      "  'sure' (8 times)\n",
      "  'course' (7 times)\n",
      "  'could' (7 times)\n"
     ]
    }
   ],
   "source": [
    "# --- Summary and Results ---\n",
    "if wer_scores:\n",
    "    avg_wer = sum(score for _, score in wer_scores) / len(wer_scores)\n",
    "    print(f\"\\nAverage WER across all files: {avg_wer:.3f}\")\n",
    "\n",
    "print(\"\\nWhispers used: Faster Whisper Base\") # You can change this manually\n",
    "\n",
    "print(\"\\nTop 10 Meaningful Word Substitution Errors:\")\n",
    "for (ref, hyp), count in meaningful_sub_counts.most_common(10):\n",
    "    print(f\"  '{ref}' → '{hyp}' ({count} times)\")\n",
    "\n",
    "print(\"\\nTop 10 Meaningful Word Insertion Errors:\")\n",
    "for word, count in meaningful_ins_counts.most_common(10):\n",
    "    print(f\"  '{word}' ({count} times)\")\n",
    "\n",
    "print(\"\\nTop 10 Meaningful Word Deletion Errors:\")\n",
    "for word, count in meaningful_del_counts.most_common(10):\n",
    "    print(f\"  '{word}' ({count} times)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05454d35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d0d111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
