{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c675a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import difflib\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "\n",
    "import jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "376787fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Folders for reference (.txt) and ASR output (.srt)\n",
    "reference_folder = \"transcripts/original_transcripts\" \n",
    "fw_asr_folder = \"transcripts/faster_whisper_transcripts\"\n",
    "wb_asr_folder = \"transcripts/whisper_base_transcripts\"\n",
    "wt_asr_folder = \"transcripts/whisper_tiny_transcripts\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72ec2fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the reference text files\n",
    "reference_files = glob(os.path.join(reference_folder, \"*.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52352f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \"\"\"Lowercase, remove punctuation, and collapse whitespace.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "690c509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_srt(srt_path):\n",
    "    \"\"\"Extracts only the dialogue text from an SRT subtitle file.\"\"\"\n",
    "    with open(srt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    text_lines = []\n",
    "    # Regex to identify timestamp lines like \"00:00:01,234 --> 00:00:05,678\"\n",
    "    timestamp_re = re.compile(r'\\d{2}:\\d{2}:\\d{2},\\d{3}\\s*-->\\s*\\d{2}:\\d{2}:\\d{2},\\d{3}')\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if re.match(r'^\\d+$', line):  # Skip subtitle number lines\n",
    "            continue\n",
    "        if timestamp_re.match(line): # Skip timestamp lines\n",
    "            continue\n",
    "        text_lines.append(line)\n",
    "        \n",
    "    return \" \".join(text_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "498f7d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Accumulators for error analysis ---\n",
    "wer_scores = []\n",
    "sub_counts = Counter()   # (reference_word, hypothesis_word) -> count\n",
    "ins_counts = Counter()   # inserted_word -> count\n",
    "del_counts = Counter()   # deleted_word -> count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8c14c5",
   "metadata": {},
   "source": [
    "### Whisper Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e73f5c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lawson-steve-20111111-stereo: WER = 0.146\n",
      "lawson-steve-20111114-stereo: WER = 0.254\n",
      "hedberg-ken-20110929-stereo: WER = 0.123\n",
      "roth-jean-starker-20071031: WER = 0.404\n",
      "lawson-steve-20110919-stereo: WER = 0.220\n",
      "lawson-steve-20110826-stereo: WER = 0.395\n",
      "white-charlie-20110518: WER = 0.342\n",
      "bella-david-20140709: WER = 0.152\n",
      "block-john-20140805: WER = 0.165\n",
      "parr-al-20140618: WER = 0.279\n",
      "robbins-bill-20120327-stereo: WER = 0.208\n",
      "hedberg-ken-20110920-stereo: WER = 0.181\n",
      "coleman-ralph-20140708: WER = 0.279\n",
      "roth-jean-starker-20071113: WER = 0.477\n",
      "mathews-chris-20110902-stereo-final: WER = 0.152\n",
      "strauss-steve-20170307: WER = 0.205\n",
      "hedberg-ken-20110909-stereo: WER = 0.169\n",
      "hedberg-ken-20111020-stereo: WER = 0.201\n"
     ]
    }
   ],
   "source": [
    "for reference_file in reference_files:\n",
    "    filename = os.path.splitext(os.path.basename(reference_file))[0]\n",
    "    asr_file = os.path.join(wb_asr_folder, filename + \".srt\")\n",
    "    \n",
    "    if not os.path.exists(asr_file):\n",
    "        print(f\"SKIPPING: ASR file missing for {filename}\")\n",
    "        continue\n",
    "\n",
    "    # --- Get Reference and Hypothesis Text ---\n",
    "    \n",
    "    # 1. Reference text from the clean .txt file\n",
    "    with open(reference_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        reference_text = f.read()\n",
    "    \n",
    "    # NEW: Remove bracketed text like [laughs] or [unintelligible]\n",
    "    reference_text = re.sub(r'\\[.*?\\]', '', reference_text)\n",
    "    \n",
    "    reference = normalize(reference_text)\n",
    "\n",
    "    # 2. Hypothesis text from the ASR's .srt file\n",
    "    hypothesis_text = extract_text_from_srt(asr_file)\n",
    "    hypothesis = normalize(hypothesis_text)\n",
    "\n",
    "    # --- Calculate WER ---\n",
    "    wer = jiwer.wer(reference, hypothesis)\n",
    "    wer_scores.append((filename, wer))\n",
    "    print(f\"{filename}: WER = {wer:.3f}\")\n",
    "\n",
    "    # --- Detailed Error Analysis using difflib ---\n",
    "    ref_words = reference.split()\n",
    "    hyp_words = hypothesis.split()\n",
    "    sm = difflib.SequenceMatcher(None, ref_words, hyp_words)\n",
    "    \n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == \"replace\":\n",
    "            for ref_word, hyp_word in zip(ref_words[i1:i2], hyp_words[j1:j2]):\n",
    "                sub_counts[(ref_word, hyp_word)] += 1\n",
    "        elif tag == \"delete\":\n",
    "            for ref_word in ref_words[i1:i2]:\n",
    "                del_counts[ref_word] += 1\n",
    "        elif tag == \"insert\":\n",
    "            for hyp_word in hyp_words[j1:j2]:\n",
    "                ins_counts[hyp_word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70bdd8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average WER across all files: 0.242\n",
      "\n",
      "Whispers used: whisper Base\n",
      "\n",
      "Top 10 Substitution Errors:\n",
      "  'in' → 'and' (233 times)\n",
      "  'a' → 'the' (140 times)\n",
      "  'the' → 'a' (112 times)\n",
      "  'and' → 'in' (103 times)\n",
      "  'and' → 'and' (75 times)\n",
      "  'was' → 'is' (69 times)\n",
      "  'the' → 'the' (64 times)\n",
      "  'pauling' → 'pauline' (51 times)\n",
      "  'i' → 'and' (51 times)\n",
      "  'it' → 'that' (47 times)\n",
      "\n",
      "Top 10 Insertion Errors:\n",
      "  'and' (2147 times)\n",
      "  'you' (700 times)\n",
      "  'the' (607 times)\n",
      "  'i' (570 times)\n",
      "  'know' (570 times)\n",
      "  'so' (554 times)\n",
      "  'that' (534 times)\n",
      "  'a' (435 times)\n",
      "  'was' (400 times)\n",
      "  'of' (369 times)\n",
      "\n",
      "Top 10 Deletion Errors:\n",
      "  'and' (476 times)\n",
      "  'of' (237 times)\n",
      "  'the' (220 times)\n",
      "  'that' (181 times)\n",
      "  'a' (176 times)\n",
      "  'it' (133 times)\n",
      "  'in' (93 times)\n",
      "  'i' (92 times)\n",
      "  'to' (82 times)\n",
      "  'you' (81 times)\n"
     ]
    }
   ],
   "source": [
    "# --- Summary and Results ---\n",
    "if wer_scores:\n",
    "    avg_wer = sum(score for _, score in wer_scores) / len(wer_scores)\n",
    "    print(f\"\\nAverage WER across all files: {avg_wer:.3f}\")\n",
    "\n",
    "print(\"\\nWhispers used: whisper Base\") # You can change this manually\n",
    "\n",
    "print(\"\\nTop 10 Substitution Errors:\")\n",
    "for (ref, hyp), count in sub_counts.most_common(10):\n",
    "    print(f\"  '{ref}' → '{hyp}' ({count} times)\")\n",
    "\n",
    "print(\"\\nTop 10 Insertion Errors:\")\n",
    "for word, count in ins_counts.most_common(10):\n",
    "    print(f\"  '{word}' ({count} times)\")\n",
    "\n",
    "print(\"\\nTop 10 Deletion Errors:\")\n",
    "for word, count in del_counts.most_common(10):\n",
    "    print(f\"  '{word}' ({count} times)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390dab7c",
   "metadata": {},
   "source": [
    "### Whisper tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a998f085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lawson-steve-20111111-stereo: WER = 0.168\n",
      "lawson-steve-20111114-stereo: WER = 0.286\n",
      "hedberg-ken-20110929-stereo: WER = 0.143\n",
      "roth-jean-starker-20071031: WER = 0.462\n",
      "lawson-steve-20110919-stereo: WER = 0.229\n",
      "lawson-steve-20110826-stereo: WER = 0.415\n",
      "white-charlie-20110518: WER = 0.408\n",
      "bella-david-20140709: WER = 0.164\n",
      "block-john-20140805: WER = 0.187\n",
      "parr-al-20140618: WER = 0.286\n",
      "robbins-bill-20120327-stereo: WER = 0.231\n",
      "hedberg-ken-20110920-stereo: WER = 0.221\n",
      "coleman-ralph-20140708: WER = 0.323\n",
      "roth-jean-starker-20071113: WER = 0.507\n",
      "mathews-chris-20110902-stereo-final: WER = 0.178\n",
      "strauss-steve-20170307: WER = 0.240\n",
      "hedberg-ken-20110909-stereo: WER = 0.193\n",
      "hedberg-ken-20111020-stereo: WER = 0.213\n"
     ]
    }
   ],
   "source": [
    "for reference_file in reference_files:\n",
    "    filename = os.path.splitext(os.path.basename(reference_file))[0]\n",
    "    asr_file = os.path.join(wt_asr_folder, filename + \".srt\")\n",
    "    \n",
    "    if not os.path.exists(asr_file):\n",
    "        print(f\"SKIPPING: ASR file missing for {filename}\")\n",
    "        continue\n",
    "\n",
    "    # --- Get Reference and Hypothesis Text ---\n",
    "    \n",
    "    # 1. Reference text from the clean .txt file\n",
    "    with open(reference_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        reference_text = f.read()\n",
    "    \n",
    "    # NEW: Remove bracketed text like [laughs] or [unintelligible]\n",
    "    reference_text = re.sub(r'\\[.*?\\]', '', reference_text)\n",
    "    \n",
    "    reference = normalize(reference_text)\n",
    "\n",
    "    # 2. Hypothesis text from the ASR's .srt file\n",
    "    hypothesis_text = extract_text_from_srt(asr_file)\n",
    "    hypothesis = normalize(hypothesis_text)\n",
    "\n",
    "    # --- Calculate WER ---\n",
    "    wer = jiwer.wer(reference, hypothesis)\n",
    "    wer_scores.append((filename, wer))\n",
    "    print(f\"{filename}: WER = {wer:.3f}\")\n",
    "\n",
    "    # --- Detailed Error Analysis using difflib ---\n",
    "    ref_words = reference.split()\n",
    "    hyp_words = hypothesis.split()\n",
    "    sm = difflib.SequenceMatcher(None, ref_words, hyp_words)\n",
    "    \n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == \"replace\":\n",
    "            for ref_word, hyp_word in zip(ref_words[i1:i2], hyp_words[j1:j2]):\n",
    "                sub_counts[(ref_word, hyp_word)] += 1\n",
    "        elif tag == \"delete\":\n",
    "            for ref_word in ref_words[i1:i2]:\n",
    "                del_counts[ref_word] += 1\n",
    "        elif tag == \"insert\":\n",
    "            for hyp_word in hyp_words[j1:j2]:\n",
    "                ins_counts[hyp_word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1a15cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average WER across all files: 0.256\n",
      "\n",
      "Whispers used: whisper Tiny\n",
      "\n",
      "Top 10 Substitution Errors:\n",
      "  'in' → 'and' (532 times)\n",
      "  'a' → 'the' (288 times)\n",
      "  'and' → 'in' (264 times)\n",
      "  'the' → 'a' (253 times)\n",
      "  'and' → 'and' (180 times)\n",
      "  'was' → 'is' (152 times)\n",
      "  'the' → 'the' (144 times)\n",
      "  'i' → 'and' (102 times)\n",
      "  'so' → 'and' (102 times)\n",
      "  'was' → 'was' (100 times)\n",
      "\n",
      "Top 10 Insertion Errors:\n",
      "  'and' (4171 times)\n",
      "  'you' (1393 times)\n",
      "  'the' (1162 times)\n",
      "  'know' (1135 times)\n",
      "  'i' (1121 times)\n",
      "  'so' (1085 times)\n",
      "  'that' (1060 times)\n",
      "  'a' (869 times)\n",
      "  'of' (748 times)\n",
      "  'was' (739 times)\n",
      "\n",
      "Top 10 Deletion Errors:\n",
      "  'and' (921 times)\n",
      "  'the' (455 times)\n",
      "  'of' (450 times)\n",
      "  'a' (382 times)\n",
      "  'that' (371 times)\n",
      "  'it' (273 times)\n",
      "  'in' (199 times)\n",
      "  'i' (182 times)\n",
      "  'you' (169 times)\n",
      "  'to' (166 times)\n"
     ]
    }
   ],
   "source": [
    "# --- Summary and Results ---\n",
    "if wer_scores:\n",
    "    avg_wer = sum(score for _, score in wer_scores) / len(wer_scores)\n",
    "    print(f\"\\nAverage WER across all files: {avg_wer:.3f}\")\n",
    "\n",
    "print(\"\\nWhispers used: whisper Tiny\") # You can change this manually\n",
    "\n",
    "print(\"\\nTop 10 Substitution Errors:\")\n",
    "for (ref, hyp), count in sub_counts.most_common(10):\n",
    "    print(f\"  '{ref}' → '{hyp}' ({count} times)\")\n",
    "\n",
    "print(\"\\nTop 10 Insertion Errors:\")\n",
    "for word, count in ins_counts.most_common(10):\n",
    "    print(f\"  '{word}' ({count} times)\")\n",
    "\n",
    "print(\"\\nTop 10 Deletion Errors:\")\n",
    "for word, count in del_counts.most_common(10):\n",
    "    print(f\"  '{word}' ({count} times)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71db8b4d",
   "metadata": {},
   "source": [
    "### Faster Whisper Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bb4dc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lawson-steve-20111111-stereo: WER = 0.140\n",
      "lawson-steve-20111114-stereo: WER = 0.250\n",
      "hedberg-ken-20110929-stereo: WER = 0.117\n",
      "roth-jean-starker-20071031: WER = 0.397\n",
      "lawson-steve-20110919-stereo: WER = 0.234\n",
      "lawson-steve-20110826-stereo: WER = 0.392\n",
      "white-charlie-20110518: WER = 0.332\n",
      "bella-david-20140709: WER = 0.126\n",
      "block-john-20140805: WER = 0.148\n",
      "parr-al-20140618: WER = 0.245\n",
      "robbins-bill-20120327-stereo: WER = 0.189\n",
      "hedberg-ken-20110920-stereo: WER = 0.167\n",
      "coleman-ralph-20140708: WER = 0.235\n",
      "roth-jean-starker-20071113: WER = 0.436\n",
      "mathews-chris-20110902-stereo-final: WER = 0.133\n",
      "strauss-steve-20170307: WER = 0.217\n",
      "hedberg-ken-20110909-stereo: WER = 0.160\n",
      "hedberg-ken-20111020-stereo: WER = 0.181\n"
     ]
    }
   ],
   "source": [
    "for reference_file in reference_files:\n",
    "    filename = os.path.splitext(os.path.basename(reference_file))[0]\n",
    "    asr_file = os.path.join(fw_asr_folder, filename + \".srt\")\n",
    "    \n",
    "    if not os.path.exists(asr_file):\n",
    "        print(f\"SKIPPING: ASR file missing for {filename}\")\n",
    "        continue\n",
    "\n",
    "    # --- Get Reference and Hypothesis Text ---\n",
    "    \n",
    "    # 1. Reference text from the clean .txt file\n",
    "    with open(reference_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        reference_text = f.read()\n",
    "    \n",
    "    # NEW: Remove bracketed text like [laughs] or [unintelligible]\n",
    "    reference_text = re.sub(r'\\[.*?\\]', '', reference_text)\n",
    "    \n",
    "    reference = normalize(reference_text)\n",
    "\n",
    "    # 2. Hypothesis text from the ASR's .srt file\n",
    "    hypothesis_text = extract_text_from_srt(asr_file)\n",
    "    hypothesis = normalize(hypothesis_text)\n",
    "\n",
    "    # --- Calculate WER ---\n",
    "    wer = jiwer.wer(reference, hypothesis)\n",
    "    wer_scores.append((filename, wer))\n",
    "    print(f\"{filename}: WER = {wer:.3f}\")\n",
    "\n",
    "    # --- Detailed Error Analysis using difflib ---\n",
    "    ref_words = reference.split()\n",
    "    hyp_words = hypothesis.split()\n",
    "    sm = difflib.SequenceMatcher(None, ref_words, hyp_words)\n",
    "    \n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == \"replace\":\n",
    "            for ref_word, hyp_word in zip(ref_words[i1:i2], hyp_words[j1:j2]):\n",
    "                sub_counts[(ref_word, hyp_word)] += 1\n",
    "        elif tag == \"delete\":\n",
    "            for ref_word in ref_words[i1:i2]:\n",
    "                del_counts[ref_word] += 1\n",
    "        elif tag == \"insert\":\n",
    "            for hyp_word in hyp_words[j1:j2]:\n",
    "                ins_counts[hyp_word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7592164d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average WER across all files: 0.246\n",
      "\n",
      "Whispers used: Faster Whisper Base\n",
      "\n",
      "Top 10 Substitution Errors:\n",
      "  'in' → 'and' (749 times)\n",
      "  'a' → 'the' (426 times)\n",
      "  'and' → 'in' (369 times)\n",
      "  'the' → 'a' (362 times)\n",
      "  'and' → 'and' (253 times)\n",
      "  'was' → 'is' (211 times)\n",
      "  'the' → 'the' (200 times)\n",
      "  'it' → 'that' (147 times)\n",
      "  'so' → 'and' (147 times)\n",
      "  'in' → 'on' (147 times)\n",
      "\n",
      "Top 10 Insertion Errors:\n",
      "  'and' (6279 times)\n",
      "  'you' (2084 times)\n",
      "  'the' (1729 times)\n",
      "  'know' (1708 times)\n",
      "  'i' (1658 times)\n",
      "  'so' (1623 times)\n",
      "  'that' (1587 times)\n",
      "  'a' (1287 times)\n",
      "  'of' (1118 times)\n",
      "  'was' (1118 times)\n",
      "\n",
      "Top 10 Deletion Errors:\n",
      "  'and' (1373 times)\n",
      "  'of' (696 times)\n",
      "  'the' (696 times)\n",
      "  'that' (572 times)\n",
      "  'a' (566 times)\n",
      "  'it' (403 times)\n",
      "  'in' (303 times)\n",
      "  'i' (282 times)\n",
      "  'you' (263 times)\n",
      "  'to' (249 times)\n"
     ]
    }
   ],
   "source": [
    "# --- Summary and Results ---\n",
    "if wer_scores:\n",
    "    avg_wer = sum(score for _, score in wer_scores) / len(wer_scores)\n",
    "    print(f\"\\nAverage WER across all files: {avg_wer:.3f}\")\n",
    "\n",
    "print(\"\\nWhispers used: Faster Whisper Base\") # You can change this manually\n",
    "\n",
    "print(\"\\nTop 10 Substitution Errors:\")\n",
    "for (ref, hyp), count in sub_counts.most_common(10):\n",
    "    print(f\"  '{ref}' → '{hyp}' ({count} times)\")\n",
    "\n",
    "print(\"\\nTop 10 Insertion Errors:\")\n",
    "for word, count in ins_counts.most_common(10):\n",
    "    print(f\"  '{word}' ({count} times)\")\n",
    "\n",
    "print(\"\\nTop 10 Deletion Errors:\")\n",
    "for word, count in del_counts.most_common(10):\n",
    "    print(f\"  '{word}' ({count} times)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05454d35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
